{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate dataset folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dog breed dataset folder includes three main folders: train, valid and test. As suggested by its name, images in folder are the direct input where the learning step bases on to estimate gradient and update the parameters. Images in the validation folder is for evaluating the accurary of the model after each epoch. The last folder, test, contains images which we will use at the last step to calculate the model's accuracy. These images are considered as the real world data, which our model has never seen, and is reliable for evaluation purpose.\n",
    "\n",
    "Because each dog breed is stored in a separate folder, we use the folder name as the label for the underlying images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset(f'dogImages/train')\n",
    "valid_files, valid_targets = load_dataset(f'dogImages/valid')\n",
    "test_files, test_targets = load_dataset(f'dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(f'dogImages/train/*/'))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both training and testing step, the input to neural network are $4D$ tensor as following:\n",
    "\n",
    "$$\n",
    "(\\text{n_samples},  \\text{rows}, \\text{columns}, \\text{channels})\n",
    "$$\n",
    "\n",
    "where `n_samples` is the total number of images, `rows` and `columns` are size and `channels` are the number of channels of each image. In our case, `channels` is 3, equivalent to  `RGB` channels.\n",
    "\n",
    "The function `path_to_tensor` takes input as a path to a single image, import, resize to the size of $(224,224)$ and then convert it to a 4D tensor, whose shape is\n",
    "$$\n",
    "(\\text{1},  \\text{rows}, \\text{columns}, \\text{channels})\n",
    "$$\n",
    "\n",
    "The function `paths_to_tensor` receives input as a list of path to images, forward each path to the function `path_to_sensor` to get a single 4D tensor, and finally, stack all of them together to build the final tensor.  \n",
    "\n",
    "We repeat calling `paths_to_tensor` to build tensors for training, valid and test sets.\n",
    "\n",
    "After loading all tensors, we divide images by 255 to convert pixel values to the range of $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [01:05<00:00, 101.82it/s]\n",
      "100%|██████████| 835/835 [00:07<00:00, 110.13it/s]\n",
      "100%|██████████| 836/836 [00:07<00:00, 111.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give it a try:  train a model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reaching out to transfer learning, I tried to train a model from scratch to  see how far I can go with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture I built below is a simplified version of VGG-11, which uses multiple continuous small convolutional layers instead of layers with large receptive field (11 or 7) like in the Alexnet architecture. Two 3x3 kernels are equavalent to a 5x5 kernel, and three 3x3 kernels are equavalent to a single 7x7 kernel, but use less parameters. Specifically, two 3x3 kernels will take  $(2*3*3*C*C) =  18*C^2 paramers $ while a single 5x5 kernel will take $5*5*C^2 parameters$. This trick gives us a gain of 28% of memory usage.\n",
    "\n",
    "In this architecture, I also incorporate 1x1 convolution layers after 3x3 ones. This 1x1 convolution layer serves as a cross-channel mapping among feature maps from previous layer, decoupling depth correlation from spatial correlation within a feature map and might give the model an easier time to learn. Moreover, it also adds non-linearity to the model, helping it learn more complex representation.\n",
    "\n",
    "Finally, I use the global average pooling layer as a bridge between the last convolution layer and the fully connected layer. GAP calculates average value of each feature map separatedly, transforming a convolution layer of size (14x14x512) to a connected layer of size (512). This kind of pooling is more native to the nature of convolution because it gives a smooth transition from feature maps to full connected layer. It also helps reduce overfitting because the average operator doesn't use any parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras as keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       65792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       262656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 5,045,125\n",
      "Trainable params: 5,045,125\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, padding='same', activation='relu', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "#model.add(Conv2D(512, kernel_size=3, padding='same', activation='relu'))\n",
    "#model.add(Conv2D(512, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/10\n",
      "6680/6680 [==============================] - 36s 5ms/step - loss: 4.9435 - acc: 0.0063 - val_loss: 4.8750 - val_acc: 0.0096\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.87504, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/10\n",
      "6680/6680 [==============================] - 31s 5ms/step - loss: 4.8770 - acc: 0.0108 - val_loss: 4.8701 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.87504 to 4.87011, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/10\n",
      "6680/6680 [==============================] - 31s 5ms/step - loss: 4.8806 - acc: 0.0096 - val_loss: 4.8689 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.87011 to 4.86890, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 4/10\n",
      "6680/6680 [==============================] - 31s 5ms/step - loss: 4.8822 - acc: 0.0112 - val_loss: 4.8693 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.86890\n",
      "Epoch 5/10\n",
      "6680/6680 [==============================] - 31s 5ms/step - loss: 4.8752 - acc: 0.0094 - val_loss: 4.8679 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00005: val_loss improved from 4.86890 to 4.86788, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 6/10\n",
      "6680/6680 [==============================] - 31s 5ms/step - loss: 4.8656 - acc: 0.0114 - val_loss: 4.8665 - val_acc: 0.0108\n",
      "\n",
      "Epoch 00006: val_loss improved from 4.86788 to 4.86645, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 7/10\n",
      "6680/6680 [==============================] - 32s 5ms/step - loss: 4.8329 - acc: 0.0156 - val_loss: 4.7909 - val_acc: 0.0204\n",
      "\n",
      "Epoch 00007: val_loss improved from 4.86645 to 4.79092, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 8/10\n",
      "6680/6680 [==============================] - 31s 5ms/step - loss: 4.7896 - acc: 0.0204 - val_loss: 4.7916 - val_acc: 0.0192\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.79092\n",
      "Epoch 9/10\n",
      "6680/6680 [==============================] - 31s 5ms/step - loss: 4.7419 - acc: 0.0175 - val_loss: 4.7140 - val_acc: 0.0204\n",
      "\n",
      "Epoch 00009: val_loss improved from 4.79092 to 4.71403, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 10/10\n",
      "6680/6680 [==============================] - 31s 5ms/step - loss: 4.6943 - acc: 0.0193 - val_loss: 4.6733 - val_acc: 0.0251\n",
      "\n",
      "Epoch 00010: val_loss improved from 4.71403 to 4.67326, saving model to saved_models/weights.best.from_scratch.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_log = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', save_best_only=True, verbose=1)\n",
    "history = model.fit(train_tensors, train_targets, epochs=10, validation_data=(valid_tensors, valid_targets), callbacks=[checkpoint_log], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find an epoch number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below graph, the change of training and validation losses over time are plotted over each other. It could be seen that, after 5 epochs, the training loss keeps decreasing but the validation loss starts increasing. It is a sign that model starts overfitting to the training set and losing its ability to generalize to the validation set. Therefore, it would be better to stop training after 5 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc6819eef0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], c='r')\n",
    "plt.plot(history.history['val_loss'], c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath='saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_tensors)\n",
    "accuracy = [np.argmax(preds[i]) == np.argmax(test_targets[i]) for i in range(preds.shape[0])]\n",
    "accuracy = 100 * (np.sum(accuracy) / len(test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 2.751196172248804%\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy = {accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even our accuracy of 2.75% is 5 times higher than a random prediction, it is still far away from a reliable  model. I guess there are several reasons behind it.\n",
    "\n",
    " 1. Our model is trained from scratch on a small dataset of just around 6000 images. For the trained model to better generalize to test images, it must be trained on large dataset of several million of images. For example, as stated in the XCeption paper, it took around one month training on a dataset of 30 million of images using a cluster of 20 GPUs.\n",
    " \n",
    " 2. The dataset just consists of dog images, while it is believed that a more diversified dataset will help the model learn more meaningul features.\n",
    " \n",
    " 3. Our architecture is not deep enough.\n",
    " \n",
    "Unfortunately, due to the limited computing resource, we are not able to train a deep architecture like VCG16 or Resnet. It would be more beneficial to apply transfer learning in this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning means we reuse a pre-trained models on our dataset. The pre-trained model are ones that are trained on big dataset and have a larger generalization ability. Choosing a appropriate approach to apply transfer learning often depends on the size and similarity between our dataset and the pre-trained model dataset.\n",
    "\n",
    "1. If our dataset is similar to the pre-trained dataset, we just need to use the pre-trained model as a feature extractor. It means that we pass our dog images through the model, take the feature maps from the last convolution layer and train our fully connected network on these feature maps. In contrast, if our dataset is different, we might need to fine-tune the pre-trained model with a low learning rate. \n",
    "\n",
    "2. Regarding the second aspect, if your dataset is small, we should consider just using the pre-trained model as a feature extractor because updating a pre-trained weight on a small dataset could cause it overfitting to the dataset. Otherwise, if we have a big dataset, we can freely fine-tune a pre-trained model.   \n",
    "\n",
    "In our case, the dataset is quite small and the model which I plan to use Resnet50 was trained on ImageNet, which is a large datase and has lots of dog images. Therefore, it could be reasonable to just use pre-trained Resnet50 as a feature extractor and train a our own fully connected network. In general, the process is as in below.\n",
    "\n",
    "1. use pre-trained Resnet50 to extract feature maps from our dog breed images.\n",
    "\n",
    "2. create a fully connected network.\n",
    "\n",
    "3. train the network using feature maps and corresponding targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
