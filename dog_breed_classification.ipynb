{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate dataset folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dog breed dataset folder includes three main folders: train, valid and test. As suggested by its name, images in folder are the direct input where the learning step bases on to estimate gradient and update the parameters. Images in the validation folder is for evaluating the accurary of the model after each epoch. The last folder, test, contains images which we will use at the last step to calculate the model's accuracy. These images are considered as the real world data, which our model has never seen, and is reliable for evaluation purpose.\n",
    "\n",
    "Because each dog breed is stored in a separate folder, we use the folder name as the label for the underlying images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset(f'dogImages/train')\n",
    "valid_files, valid_targets = load_dataset(f'dogImages/valid')\n",
    "test_files, test_targets = load_dataset(f'dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(f'dogImages/train/*/'))]\n",
    "\n",
    "n_breeds = len(dog_names)\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % n_breeds)\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both training and testing step, the input to neural network are $4D$ tensor as following:\n",
    "\n",
    "$$\n",
    "(\\text{n_samples},  \\text{rows}, \\text{columns}, \\text{channels})\n",
    "$$\n",
    "\n",
    "where `n_samples` is the total number of images, `rows` and `columns` are size and `channels` are the number of channels of each image. In our case, `channels` is 3, equivalent to  `RGB` channels.\n",
    "\n",
    "The function `path_to_tensor` takes input as a path to a single image, import, resize to the size of $(224,224)$ and then convert it to a 4D tensor, whose shape is\n",
    "$$\n",
    "(\\text{1},  \\text{rows}, \\text{columns}, \\text{channels})\n",
    "$$\n",
    "\n",
    "The function `paths_to_tensor` receives input as a list of path to images, forward each path to the function `path_to_sensor` to get a single 4D tensor, and finally, stack all of them together to build the final tensor.  \n",
    "\n",
    "We repeat calling `paths_to_tensor` to build tensors for training, valid and test sets.\n",
    "\n",
    "After loading all tensors, we divide images by 255 to convert pixel values to the range of $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image      \n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return preprocess_input(x)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6680/6680 [01:05<00:00, 102.38it/s]\n",
      "100%|██████████| 835/835 [00:07<00:00, 114.13it/s]\n",
      "100%|██████████| 836/836 [00:07<00:00, 113.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files)\n",
    "valid_tensors = paths_to_tensor(valid_files)\n",
    "test_tensors = paths_to_tensor(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give it a try:  train a model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reaching out to transfer learning, I tried to train a model from scratch to  see how far I can go with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture I built below is a simplified version of VGG-11, which uses multiple continuous small convolutional layers instead of layers with large receptive field (11 or 7) like in the Alexnet architecture. Two 3x3 kernels are equavalent to a 5x5 kernel, and three 3x3 kernels are equavalent to a single 7x7 kernel, but use less parameters. Specifically, two 3x3 kernels will take  $(2*3*3*C*C) =  18*C^2 paramers $ while a single 5x5 kernel will take $5*5*C^2 parameters$. This trick gives us a gain of 28% of memory usage.\n",
    "\n",
    "In this architecture, I also incorporate 1x1 convolution layers after 3x3 ones. This 1x1 convolution layer serves as a cross-channel mapping among feature maps from previous layer, decoupling depth correlation from spatial correlation within a feature map and might give the model an easier time to learn. Moreover, it also adds non-linearity to the model, helping it learn more complex representation.\n",
    "\n",
    "Finally, I use the global average pooling layer as a bridge between the last convolution layer and the fully connected layer. GAP calculates average value of each feature map separatedly, transforming a convolution layer of size (14x14x512) to a connected layer of size (512). This kind of pooling is more native to the nature of convolution because it gives a smooth transition from feature maps to full connected layer. It also helps reduce overfitting because the average operator doesn't use any parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras as keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       65792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       262656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 5,045,125\n",
      "Trainable params: 5,045,125\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, padding='same', activation='relu', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(256, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv2D(512, kernel_size=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2, padding='same'))\n",
    "\n",
    "#model.add(Conv2D(512, kernel_size=3, padding='same', activation='relu'))\n",
    "#model.add(Conv2D(512, kernel_size=3, padding='same', activation='relu'))\n",
    "\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_log = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', save_best_only=True, verbose=1)\n",
    "history = model.fit(train_tensors, train_targets, epochs=10, validation_data=(valid_tensors, valid_targets), callbacks=[checkpoint_log], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find an epoch number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below graph, the change of training and validation losses over time are plotted over each other. It could be seen that, after 5 epochs, the training loss keeps decreasing but the validation loss starts increasing. It is a sign that model starts overfitting to the training set and losing its ability to generalize to the validation set. Therefore, it would be better to stop training after 5 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], c='r')\n",
    "plt.plot(history.history['val_loss'], c='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath='saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_tensors)\n",
    "accuracy = [np.argmax(preds[i]) == np.argmax(test_targets[i]) for i in range(preds.shape[0])]\n",
    "accuracy = 100 * (np.sum(accuracy) / len(test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'accuracy = {accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even our accuracy of 2.75% is 5 times higher than a random prediction, it is still far away from a reliable  model. I guess there are several reasons behind it.\n",
    "\n",
    " 1. Our model is trained from scratch on a small dataset of just around 6000 images. For the trained model to better generalize to test images, it must be trained on large dataset of several million of images. For example, as stated in the XCeption paper, it took around one month training on a dataset of 30 million of images using a cluster of 20 GPUs.\n",
    " \n",
    " 2. The dataset just consists of dog images, while it is believed that a more diversified dataset will help the model learn more meaningul features.\n",
    " \n",
    " 3. Our architecture is not deep enough.\n",
    " \n",
    "Unfortunately, due to the limited computing resource, we are not able to train a deep architecture like VCG16 or Resnet. It would be more beneficial to apply transfer learning in this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning means we reuse a pre-trained models on our dataset. The pre-trained model are ones that are trained on big dataset and have a larger generalization ability. Choosing a appropriate approach to apply transfer learning often depends on the size and similarity between our dataset and the pre-trained model dataset.\n",
    "\n",
    "1. If our dataset is similar to the pre-trained dataset, we just need to use the pre-trained model as a feature extractor. It means that we pass our dog images through the model, take the feature maps from the last convolution layer and train our fully connected network on these feature maps. In contrast, if our dataset is different, we might need to fine-tune the pre-trained model with a low learning rate. \n",
    "\n",
    "2. Regarding the second aspect, if your dataset is small, we should consider just using the pre-trained model as a feature extractor because updating a pre-trained weight on a small dataset could cause it overfitting to the dataset. Otherwise, if we have a big dataset, we can freely fine-tune a pre-trained model.   \n",
    "\n",
    "In our case, the dataset is quite small and the model which I plan to use Resnet50 was trained on ImageNet, which is a large datase and has lots of dog images. Therefore, it could be reasonable to just use pre-trained Resnet50 as a feature extractor and train a our own fully connected network. In general, the process is as in below.\n",
    "\n",
    "1. use pre-trained Resnet50 to extract feature maps from our dog breed images.\n",
    "\n",
    "2. create a fully connected network.\n",
    "\n",
    "3. train the network using feature maps and corresponding targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is already packed with a pre-trained ResNet50 model, which is brought up by constructing the class ResNet50. We set parametes \"include_top\" as $False$ to tell Keras that we just want to keep the convolution layers and leave the fully connected layers out.\n",
    "\n",
    "After that, we call the method __predict__ to transform input tensors of size of (Nx224x224) to feature maps of shape of (Nx7x7x2048). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khanhhh/anaconda3/envs/tf/lib/python3.6/site-packages/keras_applications/resnet50.py:263: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras_applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "\n",
    "model_res5 = ResNet50(weights='imagenet', include_top=False)\n",
    "train_features = model_res5.predict(train_tensors)\n",
    "valid_features = model_res5.predict(valid_tensors)\n",
    "test_features = model_res5.predict(test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 7, 7, 2048)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./bottlenet_features/resnet50_train_features.npy', train_features)\n",
    "np.save('./bottlenet_features/resnet50_valid_features.npy', valid_features)\n",
    "np.save('./bottlenet_features/resnet50_test_features.npy', test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 7, 7, 2048)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_7 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x0 = Input(shape=train_features.shape[1:])\n",
    "x = GlobalAveragePooling2D()(x0)\n",
    "x = Dense(n_breeds, activation='softmax')(x)\n",
    "model = Model(x0, x)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - 1s 169us/step - loss: 0.0058 - acc: 0.9987 - val_loss: 0.9147 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00001: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - 1s 176us/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.9234 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00002: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - 1s 172us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.9052 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00003: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - 1s 174us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.8929 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00004: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 5/20\n",
      "6680/6680 [==============================] - 1s 172us/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.9113 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00005: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - 1s 174us/step - loss: 0.0047 - acc: 0.9984 - val_loss: 0.9369 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00006: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - 1s 182us/step - loss: 0.0061 - acc: 0.9990 - val_loss: 0.9634 - val_acc: 0.8287\n",
      "\n",
      "Epoch 00007: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - 1s 171us/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.9562 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00008: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - 1s 171us/step - loss: 0.0042 - acc: 0.9987 - val_loss: 1.0057 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00009: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - 1s 178us/step - loss: 0.0054 - acc: 0.9990 - val_loss: 0.9688 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00010: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - 1s 176us/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.9788 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00011: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - 1s 181us/step - loss: 0.0042 - acc: 0.9985 - val_loss: 0.9930 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00012: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - 1s 174us/step - loss: 0.0050 - acc: 0.9985 - val_loss: 1.0349 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00013: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - 1s 174us/step - loss: 0.0044 - acc: 0.9987 - val_loss: 1.0070 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00014: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - 1s 191us/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.9938 - val_acc: 0.8347\n",
      "\n",
      "Epoch 00015: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - 1s 176us/step - loss: 0.0045 - acc: 0.9988 - val_loss: 1.0081 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00016: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - 1s 168us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 1.0115 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00017: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - 1s 169us/step - loss: 0.0054 - acc: 0.9990 - val_loss: 1.0294 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00018: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - 1s 175us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 1.0155 - val_acc: 0.8335\n",
      "\n",
      "Epoch 00019: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - 1s 168us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 1.0355 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00020: saving model to ./saved_models/weights.best.resnet50_tf.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint(filepath='./saved_models/weights.best.resnet50_tf.hdf5', verbose=2)\n",
    "history = model.fit(x=train_features, y=train_targets, validation_data=(valid_features, valid_targets), epochs=epochs, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 82.41626794258373\n"
     ]
    }
   ],
   "source": [
    "n_correct_pred = np.sum(np.argmax(predicts, axis=1) == np.argmax(test_targets, axis=1))\n",
    "accuracy = (float(n_correct_pred) / len(test_targets))*100\n",
    "print('accuracy = {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
